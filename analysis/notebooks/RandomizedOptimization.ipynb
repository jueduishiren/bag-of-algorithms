{
 "metadata": {
  "name": "",
  "signature": "sha256:d8b5fb316b81be8b56c1c995fb3c146f44189de9215c4590a81d822083745338"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Randomized Optimization Algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous assighnment we analysed various supervised algorithms, which were all solving some optimization problem in the form of minimizing the derivative of the error. What if the derivative does not exists, like in discrete problems which are not defined on continuous functions? Since discrete functions cannot be differentiated, then gradient descent tool cannot be used.\n",
      "Here we turn towards search and various way of optimizing it using randomized optimization algorithms. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Genetic Algorithm on the problem \"Where is Waldo?\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I only recently came across Waldo-spotting book series (since I grew up outside of US) through the blogpost of Randy Olson where he demonstrated optimal search using genetic algorithm. This problem fascinated me by being both fun and illuminating and here I tried to reproduce some of his results using different implementation of the genetic algorithm "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from algo_evaluation.datasets import *\n",
      "from algo_evaluation.plotting import plot_waldo_data as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = load_waldo_dataset()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot_waldo_kde(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAFdCAYAAAAHT7f4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8P4fY7IJ7CK47/0AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10c063550>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Fitness function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to minimaize the distance between what waldo-looking solutions covers and the real waldo coordinates. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Genetic algorithm continuosly tinkers with the solution by slightly mutating the existing best solution until no better solution can be found."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Finding weights for ANN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section I will use optimization algorithms to search for the best weights in the Neural Network classification on the problem of Higgs detection from the previous assignment.\n",
      "\n",
      "Quick review:\n",
      "\n",
      "   - given outcomes of particle decays, detect Higgs boson; \n",
      "   \n",
      "   - most of the supervised algorithms gave acceptable accuracy ranging from 0.8 - 0.9;\n",
      "   \n",
      "   - neural network gave the worse accuracy of around 0.5 (50%) across all expreriments with tunning paramaters;\n",
      "    \n",
      "Since backpropagation algorithm in the neural network did not provide satisfactory accuracy on the higgs dataset, the problem is higgs detection becomes a great candidate to apply randomized optimization algorithms weights learning."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "higgs_data = load_higgs_train(sample_size=10000)\n",
      "features, weights, labels = higgs_data\n",
      "print 'Size of the dataset:', features.shape[0]\n",
      "print 'Number of features:', features.shape[1]\n",
      "print 'Number of positives (signal):', labels.value_counts()['s']\n",
      "print 'Number of negatives (background):', labels.value_counts()['b']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of the dataset: 2768\n",
        "Number of features: 13\n",
        "Number of positives (signal): 1270\n",
        "Number of negatives (background): 1498\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from algo_evaluation.algos import neural_network as nn\n",
      "reload(nn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "<module 'algo_evaluation.algos.neural_network' from '/Users/maestro/schoolspace/bag-of-algorithms/algo_evaluation/algos/neural_network.py'>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = nn.compare_weight_learning_optimized(higgs_data,  max_evaluation_range=xrange(1, 40, 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>algo</th>\n",
        "      <th>time</th>\n",
        "      <th>trnacc</th>\n",
        "      <th>tstacc</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max_evaluations</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 0.522377</td>\n",
        "      <td> 0.234456</td>\n",
        "      <td> 0.284180</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 0.577483</td>\n",
        "      <td> 0.003895</td>\n",
        "      <td> 0.003958</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 0.745484</td>\n",
        "      <td> 0.995883</td>\n",
        "      <td> 0.996440</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 0.922856</td>\n",
        "      <td> 0.996393</td>\n",
        "      <td> 0.995440</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 1.357351</td>\n",
        "      <td> 0.004364</td>\n",
        "      <td> 0.003137</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 1.345761</td>\n",
        "      <td> 0.003994</td>\n",
        "      <td> 0.003758</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 1.431584</td>\n",
        "      <td> 0.560696</td>\n",
        "      <td> 0.502555</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 1.788516</td>\n",
        "      <td> 0.003656</td>\n",
        "      <td> 0.004468</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.391576</td>\n",
        "      <td> 0.015933</td>\n",
        "      <td> 0.009625</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.630441</td>\n",
        "      <td> 0.846006</td>\n",
        "      <td> 0.839300</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.724544</td>\n",
        "      <td> 0.003660</td>\n",
        "      <td> 0.004480</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 3.523268</td>\n",
        "      <td> 0.413639</td>\n",
        "      <td> 0.366364</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.919957</td>\n",
        "      <td> 0.013775</td>\n",
        "      <td> 0.003998</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.593285</td>\n",
        "      <td> 0.995767</td>\n",
        "      <td> 0.996684</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.725885</td>\n",
        "      <td> 0.825755</td>\n",
        "      <td> 0.830167</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 2.895655</td>\n",
        "      <td> 0.003658</td>\n",
        "      <td> 0.004502</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 3.010054</td>\n",
        "      <td> 0.996054</td>\n",
        "      <td> 0.996143</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 4.383910</td>\n",
        "      <td> 0.003904</td>\n",
        "      <td> 0.003942</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>       hill_climbing</td>\n",
        "      <td> 4.152230</td>\n",
        "      <td> 0.268519</td>\n",
        "      <td> 0.249359</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.249584</td>\n",
        "      <td> 0.996342</td>\n",
        "      <td> 0.995553</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.243912</td>\n",
        "      <td> 0.996195</td>\n",
        "      <td> 0.995840</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.241242</td>\n",
        "      <td> 0.995971</td>\n",
        "      <td> 0.996308</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.254732</td>\n",
        "      <td> 0.980078</td>\n",
        "      <td> 0.966217</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.248605</td>\n",
        "      <td> 0.065286</td>\n",
        "      <td> 0.097131</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.241568</td>\n",
        "      <td> 0.003937</td>\n",
        "      <td> 0.003872</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.244991</td>\n",
        "      <td> 0.004114</td>\n",
        "      <td> 0.003552</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.262313</td>\n",
        "      <td> 0.095087</td>\n",
        "      <td> 0.089681</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 0.272788</td>\n",
        "      <td> 0.003938</td>\n",
        "      <td> 0.003872</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.020552</td>\n",
        "      <td> 0.004132</td>\n",
        "      <td> 0.003503</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.082950</td>\n",
        "      <td> 0.995987</td>\n",
        "      <td> 0.996287</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.489311</td>\n",
        "      <td> 0.996159</td>\n",
        "      <td> 0.995925</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.484054</td>\n",
        "      <td> 0.003957</td>\n",
        "      <td> 0.003833</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.002771</td>\n",
        "      <td> 0.305576</td>\n",
        "      <td> 0.331451</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.031503</td>\n",
        "      <td> 0.995838</td>\n",
        "      <td> 0.996533</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.122684</td>\n",
        "      <td> 0.995914</td>\n",
        "      <td> 0.996393</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.382127</td>\n",
        "      <td> 0.702732</td>\n",
        "      <td> 0.711146</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.239295</td>\n",
        "      <td> 0.995966</td>\n",
        "      <td> 0.996314</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>   genetic_algorithm</td>\n",
        "      <td> 2.049025</td>\n",
        "      <td> 0.003853</td>\n",
        "      <td> 0.004055</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 0.479335</td>\n",
        "      <td> 0.860633</td>\n",
        "      <td> 0.882922</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 0.692773</td>\n",
        "      <td> 0.003817</td>\n",
        "      <td> 0.004120</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 0.805244</td>\n",
        "      <td> 0.003925</td>\n",
        "      <td> 0.003896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 0.954966</td>\n",
        "      <td> 0.031552</td>\n",
        "      <td> 0.035602</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 1.129672</td>\n",
        "      <td> 0.996293</td>\n",
        "      <td> 0.995633</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 1.349218</td>\n",
        "      <td> 0.067193</td>\n",
        "      <td> 0.059476</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 1.507668</td>\n",
        "      <td> 0.995901</td>\n",
        "      <td> 0.996460</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 1.609670</td>\n",
        "      <td> 0.329579</td>\n",
        "      <td> 0.350097</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 1.781380</td>\n",
        "      <td> 0.995979</td>\n",
        "      <td> 0.996293</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 1.966399</td>\n",
        "      <td> 0.996149</td>\n",
        "      <td> 0.995947</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 2.255250</td>\n",
        "      <td> 0.133236</td>\n",
        "      <td> 0.137857</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 2.722331</td>\n",
        "      <td> 0.996066</td>\n",
        "      <td> 0.996120</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 2.674678</td>\n",
        "      <td> 0.996325</td>\n",
        "      <td> 0.995528</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 2.938107</td>\n",
        "      <td> 0.003881</td>\n",
        "      <td> 0.003993</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 2.968474</td>\n",
        "      <td> 0.996125</td>\n",
        "      <td> 0.996000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 3.067086</td>\n",
        "      <td> 0.004129</td>\n",
        "      <td> 0.003521</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 3.416395</td>\n",
        "      <td> 0.996074</td>\n",
        "      <td> 0.996104</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 3.928695</td>\n",
        "      <td> 0.996138</td>\n",
        "      <td> 0.995971</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> simulated_annealing</td>\n",
        "      <td> 4.649884</td>\n",
        "      <td> 0.003690</td>\n",
        "      <td> 0.004412</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "                                algo      time    trnacc    tstacc\n",
        "max_evaluations                                                   \n",
        "1                      hill_climbing  0.522377  0.234456  0.284180\n",
        "2                      hill_climbing  0.577483  0.003895  0.003958\n",
        "3                      hill_climbing  0.745484  0.995883  0.996440\n",
        "4                      hill_climbing  0.922856  0.996393  0.995440\n",
        "5                      hill_climbing  1.357351  0.004364  0.003137\n",
        "6                      hill_climbing  1.345761  0.003994  0.003758\n",
        "7                      hill_climbing  1.431584  0.560696  0.502555\n",
        "8                      hill_climbing  1.788516  0.003656  0.004468\n",
        "9                      hill_climbing  2.391576  0.015933  0.009625\n",
        "10                     hill_climbing  2.630441  0.846006  0.839300\n",
        "11                     hill_climbing  2.724544  0.003660  0.004480\n",
        "12                     hill_climbing  3.523268  0.413639  0.366364\n",
        "13                     hill_climbing  2.919957  0.013775  0.003998\n",
        "14                     hill_climbing  2.593285  0.995767  0.996684\n",
        "15                     hill_climbing  2.725885  0.825755  0.830167\n",
        "16                     hill_climbing  2.895655  0.003658  0.004502\n",
        "17                     hill_climbing  3.010054  0.996054  0.996143\n",
        "18                     hill_climbing  4.383910  0.003904  0.003942\n",
        "19                     hill_climbing  4.152230  0.268519  0.249359\n",
        "1                  genetic_algorithm  0.249584  0.996342  0.995553\n",
        "2                  genetic_algorithm  0.243912  0.996195  0.995840\n",
        "3                  genetic_algorithm  0.241242  0.995971  0.996308\n",
        "4                  genetic_algorithm  0.254732  0.980078  0.966217\n",
        "5                  genetic_algorithm  0.248605  0.065286  0.097131\n",
        "6                  genetic_algorithm  0.241568  0.003937  0.003872\n",
        "7                  genetic_algorithm  0.244991  0.004114  0.003552\n",
        "8                  genetic_algorithm  0.262313  0.095087  0.089681\n",
        "9                  genetic_algorithm  0.272788  0.003938  0.003872\n",
        "10                 genetic_algorithm  2.020552  0.004132  0.003503\n",
        "11                 genetic_algorithm  2.082950  0.995987  0.996287\n",
        "12                 genetic_algorithm  2.489311  0.996159  0.995925\n",
        "13                 genetic_algorithm  2.484054  0.003957  0.003833\n",
        "14                 genetic_algorithm  2.002771  0.305576  0.331451\n",
        "15                 genetic_algorithm  2.031503  0.995838  0.996533\n",
        "16                 genetic_algorithm  2.122684  0.995914  0.996393\n",
        "17                 genetic_algorithm  2.382127  0.702732  0.711146\n",
        "18                 genetic_algorithm  2.239295  0.995966  0.996314\n",
        "19                 genetic_algorithm  2.049025  0.003853  0.004055\n",
        "1                simulated_annealing  0.479335  0.860633  0.882922\n",
        "2                simulated_annealing  0.692773  0.003817  0.004120\n",
        "3                simulated_annealing  0.805244  0.003925  0.003896\n",
        "4                simulated_annealing  0.954966  0.031552  0.035602\n",
        "5                simulated_annealing  1.129672  0.996293  0.995633\n",
        "6                simulated_annealing  1.349218  0.067193  0.059476\n",
        "7                simulated_annealing  1.507668  0.995901  0.996460\n",
        "8                simulated_annealing  1.609670  0.329579  0.350097\n",
        "9                simulated_annealing  1.781380  0.995979  0.996293\n",
        "10               simulated_annealing  1.966399  0.996149  0.995947\n",
        "11               simulated_annealing  2.255250  0.133236  0.137857\n",
        "12               simulated_annealing  2.722331  0.996066  0.996120\n",
        "13               simulated_annealing  2.674678  0.996325  0.995528\n",
        "14               simulated_annealing  2.938107  0.003881  0.003993\n",
        "15               simulated_annealing  2.968474  0.996125  0.996000\n",
        "16               simulated_annealing  3.067086  0.004129  0.003521\n",
        "17               simulated_annealing  3.416395  0.996074  0.996104\n",
        "18               simulated_annealing  3.928695  0.996138  0.995971\n",
        "19               simulated_annealing  4.649884  0.003690  0.004412"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Running Time__:\n",
      "Evaluation three randomized algorithms for finding weights did not give much variability for running time. All algorithms are comparable and time grows linearly as number of evaluations increases.\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] Pybrain Optimization Documentation, Online Available, at http://pybrain.org/docs/tutorial/optimization.html\n",
      "\n",
      "[2] http://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf\n",
      "\n",
      "[3] http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.anneal.html\n",
      "\n",
      "[4] http://www.randalolson.com/2015/02/03/heres-waldo-computing-the-optimal-search-strategy-for-finding-waldo/"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}